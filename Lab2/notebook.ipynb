{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 3: Line Search Methods + Conjugate Gradients\n",
    "\n",
    "_Tutors: Andreas Grothey, Josh Fogg_\n",
    "\n",
    "31 January 2025\n",
    "\n",
    "This lab session experiments with the concepts of the lectures in weeks 2 and 3: line search methods and conjugate gradients. There is again an Assessed Task at the end (which follows on from Task 2(d)).\n",
    "\n",
    "## 1. Speed of convergence\n",
    "\n",
    "The first task is to investigate the speed of convergence of various iterations.\n",
    "\n",
    "### (a)\n",
    "\n",
    "The python file pi `approx.py` provides three different iterative schemes to approximate the value of $\\pi$. These can be used by (for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pi_approx import pi_iter1_arctan as pi1\n",
    "from pi_approx import pi_iter2_archim as pi2\n",
    "from pi_approx import pi_iter3_borwein as pi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions take one argument ($n$) and return the first $n$ elements of a sequence converging to $\\pi$. s a python list. The following, for example, assigns the first 10 elements of the sequence generated by `pi_iter1_arctan` to `seq` (as a `np.array`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seq = np.array(pi1(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(seq[1:11]-np.pi)/(seq[0:10]-np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to obtain the ratios $(x_{k+1} − x^∗)/(x_k − x^∗)$ for the first 10 elements of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Work out the speed of convergence for each of the three sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Have a look at the code for each function to see what calculations are involved. Would there be any circumstances where the first function (`pi_iter1_arctan`) might be preferable over the other two?\n",
    "\n",
    "## 2. Line search methods\n",
    "\n",
    "For most of this section we experiment with the files `GenericLineSearchMethod.py` and `LineSearchPlot.py`. Have a look at the file `GenericLineSearchMethod.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plots the iterates of the generic line search method for a given\n",
    "# function on the contour plot of the function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from himmelblau import himmelblau\n",
    "from rosenbrock import rosenbrock\n",
    "from ex21func import ex21\n",
    "from ex46func import ex46\n",
    "\n",
    "from GenericLineSearchMethod import GLSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines a function `GLSM` that implements a generic line search method. It takes three arguments:\n",
    "\n",
    "1. the starting point `x0` (as `np.array`),\n",
    "2. the function to be optimised `func` and\n",
    "3. a convergence tolerance `tol`.\n",
    "\n",
    "The function code starts with a parameter section where you can choose different search directions:\n",
    "\n",
    "- coordinate descent,\n",
    "- steepest gradient,\n",
    "- conjugate gradients, and\n",
    "- four line search strategies:\n",
    "  1. Exact,\n",
    "  1. Armijo,\n",
    "  2. Wolfe, and\n",
    "  3. the $1/k$ rule.\n",
    " \n",
    "Specific line search parameters such as $c_1$, $c_2$ can also be set there. The function `GLSM` returns a list of iterates encountered by the method to the calling function.\n",
    "\n",
    "Now inspect the file `LineSearchPlot.py`: Most of this should look familiar. It allows you to choose from various functions to be optimised, after setting a few function parameters (like starting point and range of $x$ and $y$ values) and plot parameters it proceeds to create a contour plot of the selected function (much in the same way as we have done in the first Lab session).\n",
    "\n",
    "### (a) Try Steepest Descent with exact line searches\n",
    "\n",
    "By default (as you download them) the files are set up to optimise the function from Example 2.1 in the lectures ($f(x, y) = x^4 +2x^3 +2x^2 +y^2 −2xy$) by using the steepest descent method with exact line searches from $x_0 = (0.5, 1)^{\\top}$. Run the python below and look at the plotted graph. You should observe that indeed subsequent search directions are perpendicular\n",
    "to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ parameters for the method --------------------------\n",
    "\n",
    "function = \"Ex21\"  # can be \"Rosenbrock\", \"Ex21\", \"Ex46\", \"Himmelblau\"\n",
    "tol = 1e-4  # tolerance for the line search method\n",
    "n = 100  # number of points for the contour discretization\n",
    "nl = 50  # number of levels for the contour plot\n",
    "\n",
    "# ------------------ end parameters for the method --------------------------\n",
    "\n",
    "if function == \"Rosenbrock\":\n",
    "    func = rosenbrock\n",
    "    x0 = np.array([0, 0])  # starting point (Rosenbrock)\n",
    "    # after this come limits for the contour plot\n",
    "    lmt_xlo, lmt_xup = -0.1, 1.1  # these are for Rosenbrock\n",
    "    lmt_ylo, lmt_yup = -0.1, 1.1\n",
    "elif function == \"Ex21\":\n",
    "    func = ex21\n",
    "    # x0 = np.array([1, 1])  # starting point (Ex2.1)\n",
    "    x0 = np.array([0.5, 1])  # starting point (Ex2.1)\n",
    "    # lmt_xlo, lmt_xup = -0.6, 1 # these are for Ex2.1 (up to nearest min)\n",
    "    # lmt_ylo, lmt_yup = -0.4, 1.2\n",
    "    lmt_xlo, lmt_xup = -1.2, 1  # these are for Ex2.1 (covering both min)\n",
    "    lmt_ylo, lmt_yup = -1.2, 1.2\n",
    "elif function == \"Ex46\":\n",
    "    func = ex46\n",
    "    x0 = np.array([2, 2])  # starting point (Ex4.6)\n",
    "    lmt_xlo, lmt_xup = -1.2, 2.1  # these are for Ex4.6\n",
    "    lmt_ylo, lmt_yup = -1.2, 2.1\n",
    "elif function == \"Himmelblau\":\n",
    "    raise ValueError(\"Himmelblau not supported yet\")\n",
    "    # func = himmelblau\n",
    "    x0 = [1, 2]  # starting point\n",
    "    lmt_xlo, lmt_xup = -1.2, 2.1  # these are for Ex4.6\n",
    "    lmt_ylo, lmt_yup = -1.2, 2.1\n",
    "else:\n",
    "    raise ValueError(\"Function code not recognized\")\n",
    "\n",
    "\n",
    "# - - - - - - - - - This is the code for the contour plot - - - - - - -\n",
    "plt.ion()\n",
    "x_values = np.linspace(lmt_xlo, lmt_xup, n)\n",
    "y_values = np.linspace(lmt_ylo, lmt_yup, n)\n",
    "X, Y = np.meshgrid(x_values, y_values)\n",
    "\n",
    "Z = func(0, X, Y)\n",
    "\n",
    "# - - uncomment this to get contours with logarithmic progression\n",
    "# lg_min = np.log((Z.min()))\n",
    "# lg_max = np.log((Z.max()))\n",
    "# levels = np.exp(np.linspace(lg_min, lg_max, 40))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "cp = ax.contour(X, Y, Z, nl)\n",
    "# cp = ax.contour(X, Y, Z, levels)   # for logarithmic progression\n",
    "\n",
    "# - - - - Calls the generic line search method and plots the path - - - -\n",
    "path = GLSM(x0, func, tol)\n",
    "ln = ax.plot(path[:, 0], path[:, 1])\n",
    "# ln = ax.plot(path[:, 0], path[:, 1],'x-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different starting points and see what happens. \n",
    "\n",
    "Inspect the function `LineSearchExact.py` that implements the exact line search. While for simple functions it is sometimes easy to derive the step length for the exact line search it is substantially more difficult to do so for a general function. The method implemented here works by seeking an $\\alpha^∗$ such that $\\nabla f(x+\\alpha^∗d)'d = 0$ (why?). It does so by first finding an interval $[\\alpha_l, \\alpha_u]$ such that $g_l = \\nabla f(x+\\alpha_{l}d)^{\\top}d < 0$ and $g_u = \\nabla f(x + \\alpha_u d)^{\\top}d > 0$ (while also requiring that $f(x + \\alpha_{l}d) < f(x)$ and $f(x + \\alpha_{u}d) < f(x)$) and then performs bisection to reduce the size of the interval. This is quite simplistic and results in a large number of necessary function evaluations. More sophisticated methods are possible, either by using second derivatives (like Newton’s method, see week 4), or various interpolations.\n",
    "\n",
    "### (b) Improve Exact Line Search\n",
    "\n",
    "One obvious place where the exact line search could be improved is in\n",
    "\n",
    "```python\n",
    "# by default just take the mid point between al and au\n",
    "am = 0.5*(al+au)\n",
    "```\n",
    "\n",
    "from `LineSearchExact.py` where as part of the bisection the midpoint between $\\alpha_l$ and $\\alpha_u$ is used as the next trial point. The situation at this point is that we have $g_l < 0$ and $g_u > 0$ and are seeking a new $\\alpha$ in $[\\alpha_l, \\alpha_u]$ with $g(\\alpha) \\approx 0$. Can you suggest a better formula (using $\\alpha_l$, $\\alpha_u$, $g_l$, $g_u$) to be used\n",
    "instead of the midpoint? Try changing the line and see what happens.\n",
    "\n",
    "### (c) Try coordinate descent\n",
    "\n",
    "Now change to coordinate descent by selecting `direction = 'CD'` at the top of `GenericLineSearchMethod.py`. Try it from the same starting point $x_0 = (0.5, 1)$. You should find that the performance is much the same as for steepest descent.\n",
    "\n",
    "### (d) Try Armijo and Wolfe line searches. Count function evaluations.\n",
    "\n",
    "Go back to the steepest descent direction (`direction = 'SD'`) and change the line search to the back-tracking Armijo line search (`linesearch = 'Armijo'` – both changes in `GenericLineSearchMethod.py`). Use a parameter of $c_1 = 0.1$ (this should be the default). Again, run the code above an observe the pattern. Also inspect the number of iterations and function evaluations necessary to find the local minimum. You should find that the number of iterations does not change much while the number of function evaluations reduces drastically. Play around with different values for the $c_1$ parameter ($c_1 = 0.9$) gives an interesting plot. Please try to explain what happens here!. **Note that the assessed task follows on from this.**\n",
    "\n",
    "If you want you can try the Bisection Line Search (for example with $c_1 = 0.1$, $x_2 = 0.5$), but the results will likely not be much different to the Armijo line search.\n",
    "\n",
    "### (e) Pre-determined step sizes\n",
    "\n",
    "As a final test on the test function `ex21` try the pre-determined step sizes (`linesearch='overk'`, \"one-over-k\"). Note that the maximal number of iterations is set to be 100 (you will hit that limit) and the final accuracy ($|g| = \\ldots$) reached by the method.\n",
    "\n",
    "### (f) Rosenbrock's function\n",
    "\n",
    "For this task change the function that is optimised to Rosenbrock's function with starting point $x_0 = (0, 0)$. For this you need to change `function='Rosenbrock'` in the code cell above. I suggest you go back to the steepest descent direction with exact line searches. Rosenbrock's function is given by\n",
    "$$\n",
    "    f(x,y) = 100{(y-x^2)}^2 + {(1-x)}^2.\n",
    "$$\n",
    "\n",
    "It is a popular (and interesting) test function for unconstrained nonlinear optimization. The shape of a function describes a steep sided valley (with the valley floor along the parabola $y = x^2)$ with a slight gradient along the valley floor making $x^∗ = (1, 1)^{\\top}$ the global minimizer (often described as a \"banana function\"). A line search method with a bad search direction will quickly hit the steep valley sides and make very slow progress. You should see that effect with the steepest descent method with exact line searches, that makes only little progress within the default maximum of 100 iterations (needing a very large number of function evaluations to do so). Using the Armijo-linesearch ($c_1 = 0.1$) is markedly better, but still does not reach the minimum within 100 iterations. You can play around with the Armijo and Wolfe line search and different parameters. Also try the pre-determined step sizes.\n",
    "\n",
    "### (g) Conjugate Gradients\n",
    "\n",
    "Finally try the Conjugate Gradient Descent method for Rosenbrock's functions (use `direction='CG'`) and see what happens.\n",
    "\n",
    "## 3. Quadratics and Eigenvalues\n",
    "\n",
    "I suspect that the tasks above will keep you busy for the lab session, but if time (or at home) you may want to try the following to get an intuitive feel of the shape of a quadratic function depending on the Hessian matrix (specifically its eigenvalues and eigenvectors).\n",
    "\n",
    "### (a)\n",
    "\n",
    "The python file `simplequadratic.py` defines a quadratic function $q(x) = \\frac{1}{2}x^{\\top}Qx + b^{\\top}x$ in the sam way as other functions we have seen last week with the addition that it takes keyword arguments `Q=...` and `b=...` (as `np.arrays`) that specify the parameters of the quadratic. That means the following piece of code will plot the contours of\n",
    "$$\n",
    "    q(x) = x^{\\top}\\begin{bmatrix}\n",
    "        3 & 2 \\\\\n",
    "        2 & 3\n",
    "    \\end{bmatrix}x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simplequadratic import simplequad as func\n",
    "\n",
    "x_values = np.linspace(-2, 2, 100)\n",
    "y_values = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x_values, y_values)\n",
    "Q = np.array([[3, 2], [2, 3]])\n",
    "Z = func(0, X, Y, Q=Q)\n",
    "fig, ax=plt.subplots(1, 1)\n",
    "ax.contour(X, Y, Z, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "You can calculate the eigenvalue and eigenvectors of the matrix $Q$ in python by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA\n",
    "w, v = LA.eig(Q)\n",
    "v0 = v[:,0]\n",
    "v1 = v[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives the eigenvalues in `w` and the two eigenvectors as `v0`, `v1`. What does that give you for the\n",
    "above matrix. Can you give a geometric interpretation of the eigenvectors?\n",
    "\n",
    "### (c)\n",
    "\n",
    "Also do this for the following matrices $Q$:\n",
    "\n",
    "$$\n",
    "    Q_2 = \\begin{bmatrix}\n",
    "        10 & 0 \\\\\n",
    "        0 & 1\n",
    "    \\end{bmatrix},\\quad Q_3 = \\begin{bmatrix}\n",
    "        3 & 1 \\\\\n",
    "        1 & 4\n",
    "    \\end{bmatrix},\\quad Q_4 = \\begin{bmatrix}\n",
    "        3 & 5 \\\\\n",
    "        5 & 4\n",
    "    \\end{bmatrix},\\quad Q_5 = \\begin{bmatrix}\n",
    "        -1 & 0 \\\\\n",
    "        0 & -2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "You may also want to try a 3d-surface plot.\n",
    "\n",
    "### (d)\n",
    "\n",
    "With the following lines of code you can plot the eigenvectors in the same diagram as the contour plot\n",
    "\n",
    "```python\n",
    "plt.arrow(0, 0, v0[0], v0[1], color=’r’, head_width=0.05)\n",
    "plt.arrow(0, 0, v1[0], v1[1], color=’r’, head_width=0.05)\n",
    "```\n",
    "\n",
    "The function `plt.arrow` takes as arguments the origin point of the arrow $(0, 0)$ and the direction of the arrow ($v_0$). Again try this for the above matrices.\n",
    "\n",
    "### (e)\n",
    "\n",
    "What changes with the geometric interpretation if $b$ is no longer equal to the zero vector?\n",
    "\n",
    "### (f)\n",
    "\n",
    "Can you give a geometric description of the form of the function\n",
    "$$\n",
    "    q(x) = \\frac{1}{2}x^{\\top}\\begin{bmatrix}\n",
    "            2 & -1 & 0 \\\\\n",
    "            -1 & 2 & 0 \\\\\n",
    "            0 & 0 & -3\n",
    "        \\end{bmatrix}x\n",
    "$$\n",
    "\n",
    "## 4. Assessed Task\n",
    "\n",
    "### (a)\n",
    "\n",
    "> Determine the rate of convergence for the steepest descent method with Armijo line search ($c_1 = 0.1$) applied to the function from Example 2.1 (this was task 2(d) above). Does it match the prediction from the theorem given in the lecture regarding the convergence rate of steepest descent (Theorem 3.4 in Nocedal/Wright).\n",
    "> \n",
    "> Note: You may find the method `np.linalg.norm(X, axis=1)` useful. It returns the 2-norm for every row in a $n\\times2$ `np.array` `X`.\n",
    "\n",
    "### (b)\n",
    "\n",
    "> Explain what happens for 2(d) and $c_1 = 0.9$.\n",
    "\n",
    "Please submit a document (or scan of a handwritten solution) that gives your answer to (a) and (b). You can include a piece of python code and its output to show what you did for part (a). You can upload this as a separate file or copy/paste it into a text/word/pdf file.\n",
    "\n",
    "Please do not submit python code as Jupyter notebook (`*.ipynb`) or any zip files (`.zip`) since these don't display inline in Learn when we are marking the assignment. Plain python (`*.py`) is fine.\n",
    "\n",
    "Submission is on the NLO Learn pages (under Assessments) by **Friday 7 February 10am**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
