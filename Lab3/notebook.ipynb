{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 3: Conjugate Gradients, Newton, and Quasi-Newton\n",
    "\n",
    "_Tutors: Andreas Grothey, Josh Fogg_\n",
    "\n",
    "14 February 2025\n",
    "\n",
    "This lab session experiments with the concepts of the lectures in weeks 3, 4 and 5: Conjugate Gradients, Newton, and Quasi-Newton.There is again an **Assessed Task** at the end.\n",
    "\n",
    "First download the python modules required for today's session from Learn (NLOLab3.zip) and unpack them. Some of the files have the same name as in the previous lab session (week 3) but extended functionality. Please work with the new versions. For most of this session we keep working with the files `GenericLineSearchMethod.py` and `LineSearchPlot.py` as in the previous lab session (week 3).\n",
    "\n",
    "## 1. Conjugate Gradients\n",
    "\n",
    "In the previous lab session we have optimized various functions by line search methods based on coordinate descent and steepest descent. Let's see if using the conjugate gradient method improves things. First try the Fletcher-Reeves variant of the CG method with exact line searches on the `ex21` problem. Go back to where the previous lab session has ended to solve the Ex21 problem with the conjugate\n",
    "gradient method\n",
    "\n",
    "### (a)\n",
    "\n",
    "For this set the function selector in `LineSearchPlot.py` to `Ex21` and the `direction` selector in GenericLineSearchMethod.py to CG (for _conjugate gradient_). Also choose the exact line search. Compare the path taken and solution statistics with the steepest descent method on the same problem. Also change the line search to the backtracking Armijo line search and see what happens.\n",
    "\n",
    "### (b) Polak-Riviere CG\n",
    "\n",
    "CG with exact line searches worked reasonably well, but the inexact Armijo line search results in a strange path (and higher number of function evaluations than for the steepest descent method). We said in the lecture that Polak-Riviere has been observed to give better performance than Fletcher-Reeves. Try that by choosing the PR method in `CGDirection.py` (change the selector `method='PR'`). First with exact line search and then with the backtracking line search.\n",
    "\n",
    "The backtracking line search stops with an error message saying that the generated direction is not a descent direction. We have seen in the lectures that this can indeed happen. A simple way to overcome this is to just use the steepest descent direction instead of CG whenever the CG direction is not a descent direction. For this change the selector `fixPR='true'` in `CGDirection.py`. This will check if the CG direction is a descent direction and if it is not return the steepest descent direction instead. Try this.\n",
    "\n",
    "NOTE: **the PR with safeguards actually converges to the other minimum!**\n",
    "\n",
    "### (c) Rosenbrock\n",
    "\n",
    "Also try use conjugate gradients on Rosenbrock's problem that had defied us earlier in week 3 (for this change the function selector in `LineSearchPlot.py`). Start with the combination CG-PR/Armijo that you have last used, but it is interesting to try CG-FR and exact line searches as well.\n",
    "\n",
    "## 2. Newton Method\n",
    "\n",
    "### (a)\n",
    "\n",
    "Change the file `GenericLineSearchMethod.py` so that it does a Newton step. Find the section that tests for `function == 'Newton'` and complete the calculation for $d_k$ (You will find the numpy function `np.linalg.solve(A, b)` useful that solves the system of equations $A{\\bf x} = {\\bf b}$). Note that you also need to change the line search strategy so that it takes full steps ($\\alpha = 1$). You can do that by setting `linesearch='fullstep'`. Once you have done it, test Newton's method on the Rosenbrock problem.\n",
    "\n",
    "### (b)\n",
    "\n",
    "While Newton converges quickly, the path taken is very surprising. Can you explain what is happening here?\n",
    "\n",
    "### (c)\n",
    "\n",
    "Change the line search to the Armijo linesearch with $c_1 = 0.1$. Observe that this now takes quite a bit longer, but is more robust. The behaviour is typical of Newton-like methods though and the indication is that Newton does better with a more lax line-search that allows occasional ascent steps. It is difficult to know how much leeway to allow it though.\n",
    "\n",
    "### (d)\n",
    "\n",
    "As the next test try Newton will full steps on problem `Ex13` from starting point $x_0 = (0.5, 1)$ (This was the problem where you determined all the stationary points in the first lab session – we have also seen this example during the lecture on Newton methods). You will see that Newton converges (quickly) to a saddle point.\n",
    "\n",
    "### (e)\n",
    "\n",
    "Change the line search to Armijo and again observe what happens.\n",
    "\n",
    "### (f)\n",
    "\n",
    "Finally you may want to try the same (Newton with full steps vs Armijo line search) from the starting point $x_0 = (0.8, 0)$: you just need to uncomment the appropriate line in `LineSearchPlot.py` under the `Ex13` section.\n",
    "\n",
    "### (g)\n",
    "\n",
    "Now try this (Newton for problem `Ex13`) from the starting point $x_0 = (−1.2, 0.5)$. You will see that Newton converges to the saddle point for both the full steps and the Armijo line search.\n",
    "\n",
    "### (h)\n",
    "\n",
    "Then try to correct inertia by calculating eigenvalues of the Hessian matrix and correcting. You can do this by opening `GenericLineSearchMethod.py` and inserting the code\n",
    "\n",
    "```python\n",
    "v, w = LA.eig(Hk)\n",
    "l_min = min(v)\n",
    "if l_min < 1e-4:\n",
    "    tau = -l_min+1e-4\n",
    "    Hk = Hk + tau*np.eye(n)\n",
    "```\n",
    "\n",
    "before calculating the Newton step with `dk = - np.linalg.solve(Hk, gk)`. Try the resulting algorithm. Also observe for how many steps the inertia correction is used.\n",
    "\n",
    "## 3. Quasi-Newton Methods\n",
    "\n",
    "Let's experiment with Quasi-Newton methods. We would like to use test problems with more than two variables. For this we can use the `chebyquad` problem that was presented in the lectures. Since we cannot plot the picture of the contour and the path taken by the method easily for higher dimensional problems we will use a different driver function namely `CallLineSearch.py` instead of `LineSearchPlot`. Have a look at its code: it is very basic and simply calls `GenericLineSearchMethod.py` for the `chebyquad` problem from a given starting point. There are a few starting points given in different dimensions. The default one is for two dimensions.\n",
    "\n",
    "Note: _The chebyquad problem finds the optimal evaluation points for a quadrature rule with equal weights on the interval $[0, 1]$._\n",
    "\n",
    "### (a)\n",
    "\n",
    "ets start with using the SR1 Quasi-Newton update rule. For this change the direction setting in `GenericLineSearchMethod.py` to `direction ='QN'`. This calls the function `QNDirection.py` to calculate the search direction using a quasi-Newton method. Again you may want to inspect this file: it allows you to choose the update rule at the top of the file; SR1 is the default.\n",
    "\n",
    "### (b)\n",
    "\n",
    "Solve the chebyquad problem from the 2-dim starting point by the SR1 update rule (with Armijo line searches) and observe what happens.\n",
    "\n",
    "### (c)\n",
    "\n",
    "For comparison try the Conjugate Gradient method (Fletcher–Reeves or Polak–Riviere) on the same problem (for this simply change the direction setting in `GenericLineSearchMethod.py` to `direction ='CG'`).\n",
    "\n",
    "This is interesting since the CG and QN methods have similar theoretical properties (n-step quadratic termination, superlinear convergence, only need gradients but no Hessians). On this example even the naive SR1-QN update performs better than conjugate gradients.\n",
    "\n",
    "### (d)\n",
    "\n",
    "Let's attempt a larger problem by changing the starting point in `CallLineSearch.py` to the 5-dimensional one. You should see that SR1 is not able to generate descent directions at some point.\n",
    "\n",
    "### (e)\n",
    "\n",
    "Change the update rule to the DFP rule and then the BFGS rule and observe what happens.\n",
    "\n",
    "### (f)\n",
    "\n",
    "Do the same for the final starting point (9-dimensional).\n",
    "\n",
    "### (g)\n",
    "\n",
    "You may want to compare with Newton.\n",
    "\n",
    "## 4. Assessed Task\n",
    "\n",
    "### (a)\n",
    "\n",
    "> For Task 3(e) above (BFGS for the 5-dimensional chebyquad problem), state the final approximation matrix $H_k$ and compare how well the inverse of $H_k$ approximates the Hessian at the solution.\n",
    "> \n",
    "> Note: _It would be sensible to compare eigenvectors and eigenvalues: for the eigenvectors for information you may want to state the angle between them; You can use $\\cos\\angle(a, b) = a^{\\top} b/{\\|a\\|}_{2}{\\|b\\|}_{2}$. Also the `np.linalg.eig` function returns eigenvectors of length 1._\n",
    "\n",
    "Please submit a document (or scan of a handwritten solution) that gives your answer to 4(a) and (b). You can include a piece of python code and its output to show what you did for part (a). You can upload this as a separate file or copy/paste it into a text/word/pdf file. Please do not submit python code as Jupyter notebook (ipynb) or any zip files (since these don't display inline in Learn when we are marking the assignment). Plain python (`*.py`) is fine. \n",
    "\n",
    "Submission is on the NLO Learn pages (under Assessments) by **Friday 1 March, 10am**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
